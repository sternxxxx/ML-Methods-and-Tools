{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Neural Network Hyperparameters\n",
    "\n",
    "We will consider an Image Recognition problem with MNIST dataset (28 x 28 images). The MNIST dataset has a training set of 60,000 images and a test set of 10,000 images. The digits have been sized-normalized and centered in a fixed-size image.\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\n",
    "\n",
    "In this Notebook you will follow two different approaches to tune the hyperparameters:\n",
    "- \"Trial and Error\" approach;\n",
    "- \"Grid Search\" Hyperparameter optimization with Scikit-Learn wrapper;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 15s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "# train_x, train_y, test_x, test_y\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29399\n"
     ]
    }
   ],
   "source": [
    "split_size = int(train_images.shape[0]*0.7)\n",
    "print(split_size)\n",
    "train_images, val_images = train_images[:split_size], train_images[split_size:]\n",
    "train_labels, val_labels = train_labels[:split_size], train_labels[split_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "\n",
    "Build a neural network with 3 layers, input, hidden and output:\n",
    "- Dense layer with 50 hidden units and an appropriate activation function;\n",
    "- Dense layer with 10 output units and an appropriate activation function;\n",
    "\n",
    "First we define some useful parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vars\n",
    "input_num_units = 784\n",
    "hidden_num_units = 50\n",
    "output_num_units = 10\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Import Keras packages that you think may need.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create the model as described above. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(input_num_units, activation='relu'))\n",
    "model.add(layers.Dense(hidden_num_units, activation='relu'))\n",
    "model.add(layers.Dense(output_num_units, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Configure the model with an optimizer and an appropriate loss function. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train the model (also with the validation set). **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_6 to have 2 dimensions, but got array with shape (29399, 10, 2, 2, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-6d28910553f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mval_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_6 to have 2 dimensions, but got array with shape (29399, 10, 2, 2, 2)"
     ]
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, batch_size = 128, epochs = 5, validation_data = (val_images, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Test the model: get some prediction and evaluate the model. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = model.predict(test_images[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 1.]\n",
      "    [1. 0.]]\n",
      "\n",
      "   [[1. 0.]\n",
      "    [0. 1.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 1.]\n",
      "    [1. 0.]]\n",
      "\n",
      "   [[1. 0.]\n",
      "    [0. 1.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 1.]\n",
      "    [1. 0.]]\n",
      "\n",
      "   [[1. 0.]\n",
      "    [0. 1.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[0. 1.]\n",
      "    [1. 0.]]\n",
      "\n",
      "   [[1. 0.]\n",
      "    [0. 1.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 1.]\n",
      "    [1. 0.]]\n",
      "\n",
      "   [[1. 0.]\n",
      "    [0. 1.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 1.]\n",
      "    [1. 0.]]\n",
      "\n",
      "   [[1. 0.]\n",
      "    [0. 1.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 1.]\n",
      "    [1. 0.]]\n",
      "\n",
      "   [[1. 0.]\n",
      "    [0. 1.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 1.]\n",
      "    [1. 0.]]\n",
      "\n",
      "   [[1. 0.]\n",
      "    [0. 1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 1.]\n",
      "    [1. 0.]]\n",
      "\n",
      "   [[1. 0.]\n",
      "    [0. 1.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[1. 0.]\n",
      "    [0. 1.]]\n",
      "\n",
      "   [[0. 1.]\n",
      "    [1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 1.]\n",
      "    [1. 0.]]\n",
      "\n",
      "   [[1. 0.]\n",
      "    [0. 1.]]]]]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "3\n",
      "3\n",
      "9\n",
      "9\n",
      "3\n",
      "9\n",
      "9\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(np.argmax(index[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to improve it by tuning some Hyperparameters.\n",
    "\n",
    "### Hyperparameters Optimization - Trial and Error\n",
    "\n",
    "Some important parameters to look out while optimizing neural networks are:\n",
    "- Type of architecture;\n",
    "- Number of layers;\n",
    "- Number of neurons per layer;\n",
    "- Regularization parameters;\n",
    "- Learning rate;\n",
    "- Type of optimization/backpropagation technique;\n",
    "- Dropout rate;\n",
    "- Weight sharing;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat all the previous steps (train, test etc..) but tuning the following parameters:\n",
    "1. Make the model \"wide\": Increase the number of neurons in the hidden layer; \n",
    "2. Make the model \"deep\": Increase the number of hidden layers neurons each;\n",
    "3. Dropout to deal with Overfitting;\n",
    "4. Increase Epochs to 50;\n",
    "5. Both \"wide\" and \"deep\": more hidden layers, each with more than 50 neurons\n",
    "\n",
    "After every step, analyse your results and draw some conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Make the model \"wide\": increase number of neurons in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Define the new variables. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vars\n",
    "input_num_units = 784\n",
    "hidden_num_units = 256\n",
    "output_num_units = 10\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build the network. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(input_num_units, activation='relu'))\n",
    "model.add(layers.Dense(hidden_num_units, activation='relu'))\n",
    "model.add(layers.Dense(output_num_units, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Configure the network. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train the network. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 5s 114us/step - loss: 0.2494 - acc: 0.9260 - val_loss: 0.1432 - val_acc: 0.9557\n",
      "Epoch 2/5\n",
      "42000/42000 [==============================] - 5s 107us/step - loss: 0.0938 - acc: 0.9715 - val_loss: 0.1009 - val_acc: 0.9699\n",
      "Epoch 3/5\n",
      "42000/42000 [==============================] - 5s 108us/step - loss: 0.0576 - acc: 0.9819 - val_loss: 0.0887 - val_acc: 0.9733\n",
      "Epoch 4/5\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.0379 - acc: 0.9883 - val_loss: 0.0998 - val_acc: 0.9714\n",
      "Epoch 5/5\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.0237 - acc: 0.9926 - val_loss: 0.0935 - val_acc: 0.9746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc644b4860>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, batch_size = 128, epochs = 5, validation_data = (val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown\n",
    "no of iterations * epoch = batch_size\n",
    "\n",
    "No major change in loss and accuracy (training and validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerations: what can you notice from these results? Is your model performing better than before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make the model \"deep\": Increase the number of hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Define the new variables. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vars\n",
    "input_num_units = 784\n",
    "hidden_num_units_1 = 50\n",
    "hidden_num_units_2 = 50\n",
    "output_num_units = 10\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build the network. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(input_num_units, activation='relu'))\n",
    "model.add(layers.Dense(hidden_num_units_1, activation='relu'))\n",
    "model.add(layers.Dense(hidden_num_units_2, activation='relu'))\n",
    "model.add(layers.Dense(output_num_units, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Configure the network. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train the network. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 5s 113us/step - loss: 0.3148 - acc: 0.9066 - val_loss: 0.1536 - val_acc: 0.9543\n",
      "Epoch 2/5\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1164 - acc: 0.9645 - val_loss: 0.1217 - val_acc: 0.9656\n",
      "Epoch 3/5\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.0705 - acc: 0.9781 - val_loss: 0.1152 - val_acc: 0.9656\n",
      "Epoch 4/5\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.0506 - acc: 0.9845 - val_loss: 0.1065 - val_acc: 0.9696\n",
      "Epoch 5/5\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.0375 - acc: 0.9888 - val_loss: 0.0895 - val_acc: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc3c2c04a8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, batch_size = 128, epochs = 5, validation_data = (val_images, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerations: what can you notice from these results? Is your model performing better than before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Define the new variables, remember to define also the dropout_ratio. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vars\n",
    "input_num_units = 784\n",
    "hidden_num_units = 50\n",
    "output_num_units = 10\n",
    "\n",
    "dropout_ratio = 0.20\n",
    "epochs = 5\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build the network. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(input_num_units, activation='relu'))\n",
    "model.add(layers.Dense(hidden_num_units, activation='relu'))\n",
    "model.add(layers.Dropout(dropout_ratio))\n",
    "model.add(layers.Dense(output_num_units, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Configure the network. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train the network. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.3465 - acc: 0.8983 - val_loss: 0.1697 - val_acc: 0.9500\n",
      "Epoch 2/5\n",
      "42000/42000 [==============================] - 4s 87us/step - loss: 0.1353 - acc: 0.9604 - val_loss: 0.1147 - val_acc: 0.9668\n",
      "Epoch 3/5\n",
      "42000/42000 [==============================] - 4s 88us/step - loss: 0.0891 - acc: 0.9743 - val_loss: 0.1087 - val_acc: 0.9671\n",
      "Epoch 4/5\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.0633 - acc: 0.9810 - val_loss: 0.0949 - val_acc: 0.9726\n",
      "Epoch 5/5\n",
      "42000/42000 [==============================] - 4s 86us/step - loss: 0.0489 - acc: 0.9840 - val_loss: 0.0980 - val_acc: 0.9704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc540699e8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, batch_size = 128, epochs = 5, validation_data = (val_images, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerations: what can you notice from these results? Is your model improving?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Increase training Epochs to 50.\n",
    "\n",
    "This will take a while."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Define the new variables. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vars\n",
    "input_num_units = 784\n",
    "hidden_num_units = 50\n",
    "output_num_units = 10\n",
    "\n",
    "dropout_ratio = 0.20\n",
    "epochs = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build the network. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(input_num_units, activation='relu'))\n",
    "model.add(layers.Dense(hidden_num_units, activation='relu'))\n",
    "model.add(layers.Dense(output_num_units, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Configure the network. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train the network. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/50\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2882 - acc: 0.9170 - val_loss: 0.1521 - val_acc: 0.9543\n",
      "Epoch 2/50\n",
      "42000/42000 [==============================] - 4s 86us/step - loss: 0.1070 - acc: 0.9686 - val_loss: 0.1202 - val_acc: 0.9638\n",
      "Epoch 3/50\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.0701 - acc: 0.9782 - val_loss: 0.0949 - val_acc: 0.9719\n",
      "Epoch 4/50\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.0422 - acc: 0.9873 - val_loss: 0.0844 - val_acc: 0.9741\n",
      "Epoch 5/50\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.0313 - acc: 0.9905 - val_loss: 0.0857 - val_acc: 0.9758\n",
      "Epoch 6/50\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.0217 - acc: 0.9938 - val_loss: 0.1007 - val_acc: 0.9730\n",
      "Epoch 7/50\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.0179 - acc: 0.9944 - val_loss: 0.0932 - val_acc: 0.9751\n",
      "Epoch 8/50\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.0157 - acc: 0.9948 - val_loss: 0.1053 - val_acc: 0.9729\n",
      "Epoch 9/50\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.0112 - acc: 0.9962 - val_loss: 0.0943 - val_acc: 0.9757\n",
      "Epoch 10/50\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.0076 - acc: 0.9978 - val_loss: 0.0953 - val_acc: 0.9768\n",
      "Epoch 11/50\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.0098 - acc: 0.9969 - val_loss: 0.1065 - val_acc: 0.9751\n",
      "Epoch 12/50\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.0087 - acc: 0.9972 - val_loss: 0.1094 - val_acc: 0.9742\n",
      "Epoch 13/50\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.0149 - acc: 0.9948 - val_loss: 0.1046 - val_acc: 0.9764\n",
      "Epoch 14/50\n",
      "42000/42000 [==============================] - 4s 88us/step - loss: 0.0096 - acc: 0.9971 - val_loss: 0.1125 - val_acc: 0.9751\n",
      "Epoch 15/50\n",
      "42000/42000 [==============================] - 4s 87us/step - loss: 0.0053 - acc: 0.9983 - val_loss: 0.1156 - val_acc: 0.9753\n",
      "Epoch 16/50\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.0068 - acc: 0.9977 - val_loss: 0.1521 - val_acc: 0.9668\n",
      "Epoch 17/50\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.0116 - acc: 0.9957 - val_loss: 0.1124 - val_acc: 0.9774\n",
      "Epoch 18/50\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.1127 - val_acc: 0.9769\n",
      "Epoch 19/50\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.1257 - val_acc: 0.9757\n",
      "Epoch 20/50\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.0133 - acc: 0.9956 - val_loss: 0.1156 - val_acc: 0.9767\n",
      "Epoch 21/50\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.0047 - acc: 0.9986 - val_loss: 0.1351 - val_acc: 0.9726\n",
      "Epoch 22/50\n",
      "42000/42000 [==============================] - 4s 88us/step - loss: 0.0076 - acc: 0.9971 - val_loss: 0.1275 - val_acc: 0.9757\n",
      "Epoch 23/50\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.0038 - acc: 0.9986 - val_loss: 0.1299 - val_acc: 0.9759\n",
      "Epoch 24/50\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.1220 - val_acc: 0.9781\n",
      "Epoch 25/50\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.1121 - val_acc: 0.9802\n",
      "Epoch 26/50\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 6.1585e-04 - acc: 0.9999 - val_loss: 0.1216 - val_acc: 0.9784\n",
      "Epoch 27/50\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.1161 - val_acc: 0.9784\n",
      "Epoch 28/50\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.0143 - acc: 0.9955 - val_loss: 0.1521 - val_acc: 0.9733\n",
      "Epoch 29/50\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.0057 - acc: 0.9980 - val_loss: 0.1144 - val_acc: 0.9799\n",
      "Epoch 30/50\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.1107 - val_acc: 0.9797\n",
      "Epoch 31/50\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.1311 - val_acc: 0.9780\n",
      "Epoch 32/50\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.0057 - acc: 0.9985 - val_loss: 0.1192 - val_acc: 0.9794\n",
      "Epoch 33/50\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.1571 - val_acc: 0.9724\n",
      "Epoch 34/50\n",
      "42000/42000 [==============================] - 4s 88us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.1350 - val_acc: 0.9767\n",
      "Epoch 35/50\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.1221 - val_acc: 0.9794\n",
      "Epoch 36/50\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 2.3719e-04 - acc: 1.0000 - val_loss: 0.1158 - val_acc: 0.9807\n",
      "Epoch 37/50\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 3.8564e-05 - acc: 1.0000 - val_loss: 0.1146 - val_acc: 0.9807\n",
      "Epoch 38/50\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 2.7106e-05 - acc: 1.0000 - val_loss: 0.1146 - val_acc: 0.9810\n",
      "Epoch 39/50\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 2.0973e-05 - acc: 1.0000 - val_loss: 0.1149 - val_acc: 0.9813\n",
      "Epoch 40/50\n",
      "42000/42000 [==============================] - 4s 88us/step - loss: 1.7433e-05 - acc: 1.0000 - val_loss: 0.1152 - val_acc: 0.9814\n",
      "Epoch 41/50\n",
      "42000/42000 [==============================] - 4s 88us/step - loss: 1.4734e-05 - acc: 1.0000 - val_loss: 0.1157 - val_acc: 0.9815\n",
      "Epoch 42/50\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 1.2550e-05 - acc: 1.0000 - val_loss: 0.1161 - val_acc: 0.9816\n",
      "Epoch 43/50\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 1.0733e-05 - acc: 1.0000 - val_loss: 0.1167 - val_acc: 0.9817\n",
      "Epoch 44/50\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 9.2259e-06 - acc: 1.0000 - val_loss: 0.1172 - val_acc: 0.9819\n",
      "Epoch 45/50\n",
      "42000/42000 [==============================] - 4s 87us/step - loss: 7.9712e-06 - acc: 1.0000 - val_loss: 0.1178 - val_acc: 0.9817\n",
      "Epoch 46/50\n",
      "42000/42000 [==============================] - 4s 88us/step - loss: 6.8737e-06 - acc: 1.0000 - val_loss: 0.1183 - val_acc: 0.9818\n",
      "Epoch 47/50\n",
      "42000/42000 [==============================] - 4s 87us/step - loss: 5.9429e-06 - acc: 1.0000 - val_loss: 0.1188 - val_acc: 0.9819\n",
      "Epoch 48/50\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 5.1557e-06 - acc: 1.0000 - val_loss: 0.1196 - val_acc: 0.9820\n",
      "Epoch 49/50\n",
      "42000/42000 [==============================] - 4s 87us/step - loss: 4.4708e-06 - acc: 1.0000 - val_loss: 0.1201 - val_acc: 0.9819\n",
      "Epoch 50/50\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 3.8718e-06 - acc: 1.0000 - val_loss: 0.1206 - val_acc: 0.9817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc2648f710>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, batch_size = 128, epochs = 50, validation_data = (val_images, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerations: what can you notice from these results? Has the accuracy increased?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Make the model \"wide\" and \"deep\": more hidden layers, each with more than 50 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Define the new variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vars\n",
    "input_num_units = 784\n",
    "hidden_num_unit_1 = 100\n",
    "output_num_units = 10\n",
    "\n",
    "epochs = 25\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build the network. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(input_num_units, activation='relu'))\n",
    "model.add(layers.Dense(hidden_num_units, activation='relu'))\n",
    "model.add(layers.Dense(hidden_num_units, activation='relu'))\n",
    "model.add(layers.Dense(hidden_num_units, activation='relu'))\n",
    "model.add(layers.Dense(hidden_num_units, activation='relu'))\n",
    "model.add(layers.Dense(hidden_num_units, activation='relu'))\n",
    "model.add(layers.Dense(output_num_units, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Configure the network. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train the network. Use 25 epochs if 50 takes too long. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/25\n",
      "42000/42000 [==============================] - 6s 140us/step - loss: 0.4029 - acc: 0.8723 - val_loss: 0.1659 - val_acc: 0.9498\n",
      "Epoch 2/25\n",
      "42000/42000 [==============================] - 5s 116us/step - loss: 0.1337 - acc: 0.9610 - val_loss: 0.1320 - val_acc: 0.9613\n",
      "Epoch 3/25\n",
      "42000/42000 [==============================] - 5s 120us/step - loss: 0.0873 - acc: 0.9736 - val_loss: 0.1417 - val_acc: 0.9577\n",
      "Epoch 4/25\n",
      "42000/42000 [==============================] - 5s 118us/step - loss: 0.0670 - acc: 0.9791 - val_loss: 0.1304 - val_acc: 0.9621\n",
      "Epoch 5/25\n",
      "42000/42000 [==============================] - 5s 123us/step - loss: 0.0491 - acc: 0.9852 - val_loss: 0.1326 - val_acc: 0.9636\n",
      "Epoch 6/25\n",
      "42000/42000 [==============================] - 5s 120us/step - loss: 0.0389 - acc: 0.9877 - val_loss: 0.0933 - val_acc: 0.9743\n",
      "Epoch 7/25\n",
      "42000/42000 [==============================] - 5s 125us/step - loss: 0.0292 - acc: 0.9913 - val_loss: 0.1039 - val_acc: 0.9725\n",
      "Epoch 8/25\n",
      "42000/42000 [==============================] - 5s 117us/step - loss: 0.0316 - acc: 0.9896 - val_loss: 0.0991 - val_acc: 0.9726\n",
      "Epoch 9/25\n",
      "42000/42000 [==============================] - 5s 115us/step - loss: 0.0265 - acc: 0.9916 - val_loss: 0.1015 - val_acc: 0.9760\n",
      "Epoch 10/25\n",
      "42000/42000 [==============================] - 5s 121us/step - loss: 0.0211 - acc: 0.9930 - val_loss: 0.1183 - val_acc: 0.9724\n",
      "Epoch 11/25\n",
      "42000/42000 [==============================] - 5s 119us/step - loss: 0.0205 - acc: 0.9933 - val_loss: 0.1134 - val_acc: 0.9748\n",
      "Epoch 12/25\n",
      "42000/42000 [==============================] - 5s 123us/step - loss: 0.0186 - acc: 0.9943 - val_loss: 0.1192 - val_acc: 0.9735\n",
      "Epoch 13/25\n",
      "42000/42000 [==============================] - 5s 123us/step - loss: 0.0168 - acc: 0.9947 - val_loss: 0.1339 - val_acc: 0.9716\n",
      "Epoch 14/25\n",
      "42000/42000 [==============================] - 6s 138us/step - loss: 0.0173 - acc: 0.9947 - val_loss: 0.1348 - val_acc: 0.9714\n",
      "Epoch 15/25\n",
      "42000/42000 [==============================] - 6s 132us/step - loss: 0.0135 - acc: 0.9957 - val_loss: 0.1177 - val_acc: 0.9745\n",
      "Epoch 16/25\n",
      "42000/42000 [==============================] - 6s 133us/step - loss: 0.0167 - acc: 0.9946 - val_loss: 0.1212 - val_acc: 0.9750\n",
      "Epoch 17/25\n",
      "42000/42000 [==============================] - 5s 131us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.1349 - val_acc: 0.9736\n",
      "Epoch 18/25\n",
      "42000/42000 [==============================] - 5s 128us/step - loss: 0.0177 - acc: 0.9947 - val_loss: 0.1483 - val_acc: 0.9713\n",
      "Epoch 19/25\n",
      "42000/42000 [==============================] - 5s 126us/step - loss: 0.0086 - acc: 0.9974 - val_loss: 0.1293 - val_acc: 0.9761\n",
      "Epoch 20/25\n",
      "42000/42000 [==============================] - 5s 122us/step - loss: 0.0136 - acc: 0.9958 - val_loss: 0.1441 - val_acc: 0.9724\n",
      "Epoch 21/25\n",
      "42000/42000 [==============================] - 5s 130us/step - loss: 0.0121 - acc: 0.9960 - val_loss: 0.1412 - val_acc: 0.9734\n",
      "Epoch 22/25\n",
      "42000/42000 [==============================] - 5s 125us/step - loss: 0.0117 - acc: 0.9963 - val_loss: 0.1292 - val_acc: 0.9762\n",
      "Epoch 23/25\n",
      "42000/42000 [==============================] - 6s 132us/step - loss: 0.0086 - acc: 0.9973 - val_loss: 0.1228 - val_acc: 0.9757\n",
      "Epoch 24/25\n",
      "42000/42000 [==============================] - 5s 131us/step - loss: 0.0081 - acc: 0.9977 - val_loss: 0.1459 - val_acc: 0.9722\n",
      "Epoch 25/25\n",
      "42000/42000 [==============================] - 5s 120us/step - loss: 0.0121 - acc: 0.9962 - val_loss: 0.1341 - val_acc: 0.9714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc25fc7978>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, batch_size = 128, epochs = 25, validation_data = (val_images, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerations: what do you think about your final model?\n",
    "It seems that we obtained our final solution model. Let's evaluate it with some predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Make some predictions and Evaluate the network. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2 = model.predict(test_images[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "2\n",
      "1\n",
      "0\n",
      "4\n",
      "1\n",
      "4\n",
      "9\n",
      "6\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(np.argmax(index_2[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Optimization - Grid Search\n",
    "\n",
    "Instead of proceeding with a \"trial and error\" approach we can also use GridSearch to combine all the hyperparameters we want to tune, or some of them. What you have to do is to use Sequential() model in Keras as a part of the Scikit-Learn workflow via the wrappers. \n",
    "Check out how this workflow works.\n",
    "\n",
    "\n",
    "Please note that without GPU is extremely time consuming to tune all the hyperparameters in one shoot, by using an appropriate number of epochs. For this reason, in this example the idea is for you to understand how you can use GridSerach with Keras Model, but you will probably not be able to obtain an excellent model.\n",
    "\n",
    "For this reason, try to tune the number of neurons in the hidden layers (more than one hidden layer) with just 5-10 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Import GridSearchCV and KerasClassifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_num_units = 784\n",
    "hidden_num_unit = 100\n",
    "output_num_units = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a function called `create_model` in which you build your KerasClassifier with number of hidden units equal to a general variable (you can call this `neurons`). Inside the function you should then also compile the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(neurons):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(input_num_units, activation='relu'))\n",
    "    model.add(layers.Dense(neurons, activation='relu'))\n",
    "    model.add(layers.Dense(neurons, activation='relu'))\n",
    "    model.add(layers.Dense(neurons, activation='relu'))\n",
    "    model.add(layers.Dense(neurons, activation='relu'))\n",
    "    model.add(layers.Dense(neurons, activation='relu'))\n",
    "    model.add(layers.Dense(output_num_units, activation='softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a model wrapper.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = KerasClassifier(create_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a dictionary of parameters grid for the number of neurons in the hidden layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " param_grid={'dense_layer_sizes': dense_size_candidates,\n",
    "                                     # epochs is avail for tuning even when not\n",
    "                                     # an argument to model building function\n",
    "                                     'epochs': [5, 10],\n",
    "                                     'filters': [8],\n",
    "                                     'kernel_size': [3],\n",
    "                                     'pool_size': [2]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Grid Search: use `GridSearchCV`with the model you have obtained from the wrapper as estimator and the dictionary you have just created as param_grid.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Fit the result from the Grid Search, call your result `grid_result`. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's print some results. Fill in the #TO DOs with the best_score and best_params that you got after fitting. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (#TO DO: take the best score, #TO DO: take the best parameters) \n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
